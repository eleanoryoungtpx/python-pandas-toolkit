{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "There are many problems that often occur in raw data such as inconsistencies, missing values and duplicate entries. We want to tackle these issues to make the data easier to analyse. In this tutorial, we will explore the common techniques used in pandas to clean and preprocess data. These include handling missing values, detecting and removing duplicates, converting data types and tidying up string entries. \n",
    "\n",
    "This section will cover key functions:\n",
    "- Handling null values:  `isnull`, `dropna`, `ffill`\n",
    "- Handling duplicate values:  `duplicated`, `drop_duplicates`\n",
    "- Handling data types:  `astype`, `to_numeric`\n",
    "- String operations:  `str`, `replace`, `re`\n",
    "\n",
    "## Identify and handle missing data\n",
    "\n",
    "You can identify if a row contains any null values using `isnull.any(axis=1)`, the `axis=1` means we are looking at rows and `any` returns if there are any true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header=2)\n",
    "\n",
    "# Print rows that contain null values\n",
    "null_rows = data[data.isnull().any(axis=1)]\n",
    "\n",
    "null_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar approach for identifying columns containing nul values, where the default axis in the `any` function is `axis=0` meaning columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns that contain null values\n",
    "null_columns = data.columns[data.isnull().any()].tolist()\n",
    "\n",
    "null_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can drop null values using `dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_nulls = data.dropna()\n",
    "\n",
    "null_rows = data_no_nulls[data_no_nulls.isnull().any(axis=1)]\n",
    "\n",
    "print(null_rows)\n",
    "print(data_no_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to provide a threshold for `dropna` this is the number of columns containing null that you will then drop. Often when reading in xlsx files it will include notes that happen below the actual data, so applying a threshold to `dropna` is very useful for getting rid of these rows, as these rows will usually contain 1 column with data and the rest null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(thresh=1)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are scenarios were instead of removing these entries completely you wish to replace it with something else. You can do so using `fillna`. So here we are considering discounts so it is reasonable to replace null by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header=2)\n",
    "data = data.fillna(0)\n",
    "\n",
    "print(data[data[\"Discounts\"] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common situation I have uncountered is where we have a column for year and quarter and because the data is in order only Q1 has a year. In this scenario we want to do a forward fill `ffill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Year': [2000, None, None, None, 2001, None, None, None, 2002, None, None, None, 2003, None, None],\n",
    "    'Quarter': ['Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df.ffill()\n",
    "\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simiarly there is also a backward fill function `bfill`. Please see documentation https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and handle duplicate data\n",
    "\n",
    "You can identify duplicates using the uplicates function `duplicated`. By default the function checks duplicates across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path.cwd() / \"data/customers-1000.csv\")\n",
    "\n",
    "duplicates = data[data.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains no duplicate values. You can specify across what columns you want to check for duplicates by listing the columns you want to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data[data.duplicated([\"City\"])]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates you can use the `drop_duplicates` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[\"Country\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and handle data types\n",
    "\n",
    "Data types are very important as it will effect the functions you can use on your data, it is possible that python has misinterpretated your data as something else. For example for many things we will look at later, we need to ensure that numeric data is interpreted as numeric in order to use it for plotting charts and using a `pivot_table`. The `info` function can be really useful for getting some overview information about a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", usecols=[0,1,2,4], header =2)\n",
    "\n",
    "print(data)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data types we will look at are:\n",
    "   - `int`: Represents integers (e.g., `x = 10`)\n",
    "   - `float`: Represents floating-point numbers (e.g., `x = 3.14`)\n",
    "   - `str`: String, sequence of characters (e.g., `text = \"Hello, World!\"`)\n",
    "   - `bool`: Boolean value (`True` or `False`, e.g., `flag = True`)\n",
    "   - `NoneType`: Represents the absence of value (e.g., `x = None`)\n",
    "\n",
    "We can set the data types by either specifying it when we read in the data or later we can apply `astype` to a column. The Object is the default for columns of strings and mixed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    \"Customer Id\": str}\n",
    "\n",
    "data  = pd.read_csv(Path.cwd() / \"data/customers-1000.csv\", usecols=[0,1,2], dtype=dtype_dict)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then enforce it as a string by applying `astype` to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"First Name\"] = data[\"First Name\"].astype(str)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"First Name\"] = data[\"First Name\"].astype(\"string\")\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful pandas function is `to_numeric` this converts a column to numeric types (`integer` or `float`). It can handle possible non-numeric values like strings or null values. The defaults is `errors=\"raise\"` which will raise an error when there are non-numeric values but more usefully `errors=\"coerce\"` will convert non-numeric values to nan, this is useful when raw data contains values such as \"-\" when data is missing or surpressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'col1': ['10', '20', 'thirty', '40', 'NaN']})\n",
    "\n",
    "# Convert to numeric using pd.to_numeric(), non-numeric values will become NaN\n",
    "data['col1_numeric'] = pd.to_numeric(data['col1'], errors='coerce')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`astype` is less flexible as it will error when you try convert strings or nulls to a number but you may wish to be alerted to non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['col1_numeric'] = data['col1'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give an error. So in conclusion astype is useful for establishing types when all the data can easily be numeric data but `to_numeric` gives you a nice way to dealing with invalid data and quickly handle errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "String operations are incredibly useful for cleaning up data or putting things into a particular format. One useful operation you can do with strings is character slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header =2, usecols=[14,15])\n",
    "\n",
    "# You must have str entries to apply character slicing\n",
    "data[\"Year\"] = data[\"Year\"].astype(str)\n",
    "\n",
    "# This combines the first 3 characters from \"Month Name\" and last 2 characaters from \"Year\"\n",
    "data[\"Date\"] = data[\"Month Name\"].str[:3] + \"-\" + data[\"Year\"].str[-2:]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str` accessor allows you to perform string operations in a vectroized manner, like here applying the operation to a whole column. Another extremely useful string operation is the `replace` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date\"] = data[\"Date\"].str.replace(\"-\",\" \")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more unique situations, using regex can be extremely useful for extracting particular information. I find this to be a really good use of chatgpt is to provide regex code. An example of this would be extracting the year of the data from a url. Using regex in python requires importing the `re` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example URL\n",
    "url = \"https://example.com/2021/learn-how-to-extract-year\"\n",
    "\n",
    "# Find any 4-digit number between 1900 and 2024\n",
    "match = re.search(r'/([1-2][0-9]{3})/', url)\n",
    "\n",
    "# Check if it's within the range 1900-2024 and print the result\n",
    "year = match.group(1) if match and 1900 <= int(match.group(1)) <= 2024 else \"No valid year found\"\n",
    "print(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
