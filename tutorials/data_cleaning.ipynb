{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "There are many problems that often occur in raw data such as inconsistencies, missing values and duplicate entries. We want to tackle these issues to make the data easier to analyse. In this tutorial, we will explore the common techniques used in pandas to clean and preprocess data. These include handling missing values, detecting and removing duplicates, converting data types, working with categorical and string data, and performing basic data transformations. \n",
    "\n",
    "This section will cover key functions:\n",
    "- Handling null values:  `isnull`, `dropna`, `ffill`\n",
    "- Handling duplicate values:  `duplicated`, `drop_duplicates`\n",
    "- Handling data types:  `astype`, `to_numeric`\n",
    "- String operations:  `str`, `replace`, `re`\n",
    "\n",
    "## Identify and handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header=2)\n",
    "\n",
    "# Print rows that contain null values\n",
    "null_rows = data[data.isnull().any(axis=1)]\n",
    "\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns that contain null values\n",
    "null_columns = data.columns[data.isnull().any()].tolist()\n",
    "\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can drop null values using `dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_nulls = data.dropna()\n",
    "\n",
    "null_rows = data_no_nulls[data_no_nulls.isnull().any(axis=1)]\n",
    "\n",
    "print(null_rows)\n",
    "print(data_no_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to provide a threshold for `dropna` this is the number of columns containing null that you will then drop. Often when reading in xlsx files it will include notes that happen below the actual data, so `dropna` is very useful for getting rid of these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are scenarios were instead of removing these entries completely you wish to replace it with something else. You can do so using `fillna`. So here we are considering discounts so it is reasonable to replace null by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header=2)\n",
    "data = data.fillna(0)\n",
    "\n",
    "print(data[data[\"Discounts\"] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common situation I have uncountered is where we have a column for year and quarter and because the data is in order only Q1 has a year. In this scenario we want to do a forward fill `ffill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Year': [2000, None, None, None, 2001, None, None, None, 2002, None, None, None, 2003, None, None],\n",
    "    'Quarter': ['Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3', 'Q4', 'Q1', 'Q2', 'Q3']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df.ffill()\n",
    "\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simiarly there are other fill functions such as `bfill`. Please see documentation https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and handle duplicate data\n",
    "\n",
    "You can identify duplicates using built in duplicates function. By default the function checks duplicates across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path.cwd() / \"data/customers-1000.csv\")\n",
    "\n",
    "duplicates = data[data.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains no duplicate values. You can specify across what columns you want to check for duplicates by listing the columns you want to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data[data.duplicated([\"City\"])]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates you can use the built in `drop_duplicates` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[\"Country\"])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and handle data types\n",
    "\n",
    "Data types are very important as it will effect the functions you can use if python has misinterpretated your data as something else. For example for many things we will look at later, we need to ensure that numeric data is interpreted as numeric in order to use it for plotting charts and using `pivot_table`. The `info` function can be really useful for getting some overview information aboout a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", usecols=[0,1,2,4], header =2)\n",
    "\n",
    "print(data)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data types we will look at are:\n",
    "   - `int`: Represents integers (e.g., `x = 10`)\n",
    "   - `float`: Represents floating-point numbers (e.g., `x = 3.14`)\n",
    "   - `str`: String, sequence of characters (e.g., `text = \"Hello, World!\"`)\n",
    "   - `bool`: Boolean value (`True` or `False`, e.g., `flag = True`)\n",
    "   - `NoneType`: Represents the absence of value (e.g., `x = None`)\n",
    "\n",
    "We can set the data types by either specifying it when we read in the data or later we can apply `astype` to a column. Object us the default for strings and mixed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    \"Customer Id\": str}\n",
    "\n",
    "data  = pd.read_csv(Path.cwd() / \"data/customers-1000.csv\", usecols=[0,1,2], dtype=dtype_dict)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then enforce it as a string by applying as type to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"First Name\"] = data[\"First Name\"].astype(str)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"First Name\"] = data[\"First Name\"].astype(\"string\")\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful pandas function is to_numeric this converts a column to numeric types (`integer` or `float`). It can handle possible non-numeric values like strings or null values. The defaults is `errors=\"raise\"` which will error when are non-numeric values but more usefully `errors=\"coerce\"` will convert non-numeric values to nan, this is useful when raw data contains values such as \"-\" when data is missing or surpressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'col1': ['10', '20', 'thirty', '40', 'NaN']})\n",
    "\n",
    "# Convert to numeric using pd.to_numeric(), non-numeric values will become NaN\n",
    "data['col1_numeric'] = pd.to_numeric(data['col1'], errors='coerce')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['col1_numeric'] = data['col1'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give an error. So in conclusion astype is useful so establishing types when all the data can easily be numeric data but `to_numeric` gives you a nice way to dealing with invalid data and quickly handle errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "String operations are incredibly useful for cleaning up data or putting things into a particular format. One useful operation you can do with strings is character slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(Path.cwd() / \"data/Financial Sample.xlsx\", header =2, usecols=[14,15])\n",
    "\n",
    "# You must have str entries to apply character slicing\n",
    "data[\"Year\"] = data[\"Year\"].astype(str)\n",
    "\n",
    "# This combines the first 3 characters from \"Month Name\" and last 2 characaters from \"Year\"\n",
    "data[\"Date\"] = data[\"Month Name\"].str[:3] + \"-\" + data[\"Year\"].str[-2:]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str` accessor allows you to perform string operations in a vectroized manner like here applying the operation to a whole column. Another extremely useful string operation is the replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date\"] = data[\"Date\"].str.replace(\"-\",\" \")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more unique situations, using regex can be extremely useful for extracting particular information. I find this to be a really good use of chatgpt is to provide regex code. An example of this would be extracting the year of the data from a url. Using regex in python requires `re` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example URL\n",
    "url = \"https://example.com/2021/learn-how-to-extract-year\"\n",
    "\n",
    "# Find any 4-digit number between 1900 and 2024\n",
    "match = re.search(r'/([1-2][0-9]{3})/', url)\n",
    "\n",
    "# Check if it's within the range 1900-2024 and print the result\n",
    "year = match.group(1) if match and 1900 <= int(match.group(1)) <= 2024 else \"No valid year found\"\n",
    "print(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
