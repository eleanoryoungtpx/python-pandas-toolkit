{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "We will look at different ways to read data such as directly from a file or from a url. We will consider some popular file types and different argument options for reading these. This section will cover key functions and packages for reading data:\n",
    "\n",
    "- Getting filepaths: `pathlib`\n",
    "- Reading local files: `read_csv`, `read_excel`\n",
    "- Reading data from urls: `requests`, `BeautifulSoup`\n",
    "\n",
    "\n",
    "You must first start by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access pandas functions by using `pd.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting filepaths\n",
    "\n",
    "For reading data from files you have saved locally on your machine or on the repositorty (not recommended for real data) it is useful to use the `Path` package to automatically give you part of the filepath. This is preferred over hardcoding full filepaths. When using the `Path` type you join filepaths using `/`. Whereas if you are writing out the path as a string, you join paths using `+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "home_directory = Path.home()\n",
    "documents_directory = Path.home() / \"Documents\"\n",
    "\n",
    "print(current_directory)\n",
    "print(home_directory)\n",
    "print(documents_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note for `Path.cwd()` returns the current working directory. If you are using a Jupyter Notebook (as we currently are) this will give you the filepath to where the Jupyter Notebook is located. Whereas if you are using a .py file this will give the filepath to where you are in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV files\n",
    "\n",
    "We will start with the most simple case - reading CSV files. The example data can be found in the repository in the `tutorials/data` folder. We can use pandas function `read_csv` for reading CSV files where the only argument we need to supply is the filepath to the CSV we wish to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(current_directory / \"data/customers-1000.csv\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular arguments\n",
    "\n",
    "**usecols** - allows you select only certain columns to be read in. Either by putting the column names in a list or by referencing there positional argument (remember python indexes from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(current_directory / \"data/customers-1000.csv\", usecols=[\"Customer Id\", \"First Name\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(current_directory / \"data/customers-1000.csv\", usecols=[1,2])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many possible arguments but I don't often find the need to use these for CSV's. For more information on the different arguments, please reference the documentation https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading XLSX\n",
    "\n",
    "Now we will consider reading data from xlsx or xls file. We use the pandas function `read_excel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(current_directory / \"data/Financial Sample.xlsx\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular arguments\n",
    "\n",
    "**header** - allows you pick which row number you want to use as your column headers (again remember python indexs from 0)\n",
    "\n",
    "**usecols** - allows you select only certain columns to be read in. Either by putting the column names in a list or by referencing there positional argument or can also reference the column letters from excel as a string. Can use \":\" to mean a range. For example, \"A,C\" means column \"A\" and \"C\", \"A:C\" means column \"A,B,C\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(current_directory / \"data/Financial Sample.xlsx\", header=2, usecols=[\"Segment\",\"Country\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(current_directory / \"data/Financial Sample.xlsx\", header=2, usecols=\"A:C,F,H\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sheet_name** - usually the xlsx file will contain multiple sheets so simply state name of the sheet. Is only required when the file has multiple sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(current_directory / \"data/Financial Sample.xlsx\", sheet_name=\"Sheet1\", header=2, usecols=\"A:C,F,H\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encoding** - the default encoding is `utf-8` this will usually work but sometimes you may get an error that it is unable to read the file due to encoding and you must try another type, such as `ISO-8859-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One horrible scenario you may encounter with xlsx files is that cells are merged or a single sheet contains multiple tables. Please see the example below for dealing with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "        current_directory / \"data/Financial Sample merged cells.xlsx\", header=[2,3])\n",
    "\n",
    "# Get the number of columns\n",
    "num_cols = df.shape[1]\n",
    "\n",
    "# Extract merged headers and column names\n",
    "merged_headers = df.columns.get_level_values(0)\n",
    "column_names = df.columns.get_level_values(1)\n",
    "\n",
    "# Initialize the current merged header\n",
    "current_merged_header = None\n",
    "\n",
    "# Create new column headers\n",
    "new_columns = []\n",
    "for col in range(num_cols):\n",
    "    merged_header = merged_headers[col]\n",
    "    column_name = column_names[col]\n",
    "    \n",
    "    # Update the current merged header if it's not empty\n",
    "    if pd.notna(merged_header) and merged_header != '':\n",
    "        current_merged_header = merged_header\n",
    "    \n",
    "    # Combine the current merged header with the column name\n",
    "    if current_merged_header:\n",
    "        new_column_name = f\"{current_merged_header}_{column_name}\"\n",
    "    else:\n",
    "        new_column_name = column_name\n",
    "    \n",
    "    new_columns.append(new_column_name)\n",
    "\n",
    "# Assign the new column headers to the DataFrame\n",
    "df.columns = new_columns\n",
    "\n",
    "# Filter out the blank columns between tables\n",
    "df = df.loc[:, ~df.columns.str.contains('Date.1', case=False, na=False)]\n",
    "\n",
    "# Drop the header rows (first two rows)\n",
    "df = df.drop(index=[0, 1])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading ODS files\n",
    "\n",
    "This works very similar to reading excel files, the only difference being that you must state an appropriate engine to deal with this as an argument. I have found the fastest to be `engine=\"calamine\"`. More information on engines can be found here in the `read_excel` documentation https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from urls\n",
    "\n",
    "To read data directly from urls we use the `requests` package. We will be looking at downloading data from this url https://public.tableau.com/app/learn/sample-data\n",
    "\n",
    "The first example will look at providing full url to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://public.tableau.com/app/sample-data/EMSI_JobChange_UK.xlsx\"\n",
    "\n",
    "# Send request to get data, allows 10 seconds or there will be a timeout error\n",
    "data = requests.get(url, timeout=10)\n",
    "\n",
    "# Use BytesIO to handle the content in memory without need to save it \n",
    "file_content = BytesIO(data.content)\n",
    "    \n",
    "# Read excel file into dataframe\n",
    "df = pd.read_excel(file_content, sheet_name=\"1 digit\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may only want to provide the url to the landing page then use python to access functionality of the page such as a download button. We can use the package `BeautifulSoup` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url_landing = \"https://public.tableau.com/app/learn/sample-data\"\n",
    "page = requests.get(url_landing, timeout=10)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    if \"Dataset (xlsx)\" in str(a):\n",
    "        url = a[\"href\"]\n",
    "        break\n",
    "    \n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then returns you the full url and you can read into a pandas dataframe as before. So the complete code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_landing = \"https://public.tableau.com/app/learn/sample-data\"\n",
    "page = requests.get(url_landing, timeout=10)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    if \"Dataset (xlsx)\" in str(a):\n",
    "        url = a[\"href\"]\n",
    "        break\n",
    "    \n",
    "# Send request to get data, allows 10 seconds or there will be a timeout error\n",
    "data = requests.get(url, timeout=10)\n",
    "\n",
    "# Use BytesIO to handle the content in memory without need to save it \n",
    "file_content = BytesIO(data.content)\n",
    "    \n",
    "# Read excel file into dataframe\n",
    "df = pd.read_excel(file_content, sheet_name=\"1 digit\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing dataframes\n",
    "\n",
    "Sometimes you may not want to print the whole dataframe. There are plenty of useful functions to give you a quick view of your data. To only view the first 5 rows of the dataframe we can use the `.head()` function, similarly to view the bottom 5, we can use `.tail()`. The default is 5 but you can also specify how many rows you wish to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print a number of rows based on the size. So if you want the top 10 with the values in the 'Change' column, you can use the `nlargest` function where the first argument is number of rows and second is what column you want to sort by. Similar approach can be done using the `nsmallest` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nlargest(10,\"Change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a quick overview of data types and size of Dataframe using the `.info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all the column headers using `.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
